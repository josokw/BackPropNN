# Back Propagation Neural Network

- The goal of any **supervised learning algorithm** is to find a function that best maps a set of inputs to their correct output. The motivation for backpropagation is to train a multi-layered neural network such that it can learn the appropriate internal representations to allow it to learn any arbitrary mapping of input to output. See [Wikipedia](https://en.wikipedia.org/wiki/Backpropagation) and [Neural Network Architectures & Deep Learning](https://www.youtube.com/results?search_query=neural+network+types+overview).
  
- **Neuron/perceptron**: the basic unit of the neural network. Accepts an input and generates a prediction. Neural networks generate their predictions in the form of a set of real values or boolean decisions. Each output value is generated by one of the neurons in the output layer.

- **Activation function** example: non-linear **Hyperbolic Tangent**, zero centered making it easier to model inputs that have strongly negative, neutral, and strongly positive values. See video [Types of activation functions](https://www.youtube.com/watch?v=Fu273ovPBmQ).
  
## Simulation

Training sets: XOR, OR, AND, AND3, NAND and more can  be found in text files in the *data* sub directory.

## Refactoring to Modern C++

Source code: [David Miller, C++ code example](https://inkdrop.net/dave/docs/neural-net-tutorial.cpp), also available in *src-original* directory.
Associated video: [David Miller, Neural Net in C++ Tutorial](https://vimeo.com/19569529)
Goal of this project: refactoring the David Miller example code to Modern C++. Still under construction ...

[![Codacy Badge](https://api.codacy.com/project/badge/Grade/2cd688b1e3984f63b00fdee04e7dac4b)](https://www.codacy.com/project/josokw/BackPropNN/dashboard?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=josokw/BackPropNN&amp;utm_campaign=Badge_Grade_Dashboard)
[![CodeFactor](https://www.codefactor.io/repository/github/josokw/backpropnn/badge)](https://www.codefactor.io/repository/github/josokw/backpropnn)

## Compiling

The C++ code needs **c++20**.

Go to the *build* directory and type:

```bash
cmake ..
make -j
```

## Running and training a BackProp NN

A set of inputs for which the correct outputs are known, used to train the neural network.

Training **XOR**, topology:

- 2 inputs
- 1 hidden layer 5 neurons
- 1 output neuron

Empty lines and single line comments after **#** are allowed.

The training parameters *momentum* and *learning_rate* are optional.
If not used default values are used.

```txt
# trainingXOR.txt

momentum: 0.5
learning_rate: 0.15

topology: 2      5    1
actionfs: inputs tanh tanh

in: 0.0 0.0
out: 0.0

in: 1.0 0.0
out: 1.0

in: 0.0 1.0
out: 1.0

in: 1.0 1.0
out: 0.0
```

Run the code for training XOR in the *bin* directory:

```bash
./backpropnn ../data/trainingXOR.txt
```
